{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# mount google drive and switch project root dir","metadata":{"id":"VpFmHFFvUdhj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install facenet-pytorch --quiet","metadata":{"id":"L49WF_xGw4vI","outputId":"0fa3ae8c-9ec9-4fd8-e089-65e94aa89b25","execution":{"iopub.status.busy":"2022-07-26T08:13:54.646985Z","iopub.execute_input":"2022-07-26T08:13:54.647816Z","iopub.status.idle":"2022-07-26T08:14:07.194933Z","shell.execute_reply.started":"2022-07-26T08:13:54.647771Z","shell.execute_reply":"2022-07-26T08:14:07.193819Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"! conda install -y gdown","metadata":{"execution":{"iopub.status.busy":"2022-07-26T08:12:46.155594Z","iopub.execute_input":"2022-07-26T08:12:46.156263Z","iopub.status.idle":"2022-07-26T08:13:54.644629Z","shell.execute_reply.started":"2022-07-26T08:12:46.156226Z","shell.execute_reply":"2022-07-26T08:13:54.643484Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# imports\nimport os\nimport gc\nimport csv\nimport glob\nimport pandas as pd\nimport PIL\nimport h5py\nimport multiprocessing\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data import Dataset\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nimport os.path as osp\n\nfrom facenet_pytorch import MTCNN, InceptionResnetV1","metadata":{"id":"taKuFbCeceL0","execution":{"iopub.status.busy":"2022-07-26T08:14:07.196565Z","iopub.execute_input":"2022-07-26T08:14:07.197353Z","iopub.status.idle":"2022-07-26T08:14:07.378711Z","shell.execute_reply.started":"2022-07-26T08:14:07.197322Z","shell.execute_reply":"2022-07-26T08:14:07.377717Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#from google.colab import drive\n#drive.mount('/content/drive')\n#os.chdir('/content/drive/MyDrive/projects/NMA-Transfer_Learning')\n#print(os.getcwd())\n#print(os.listdir())\n\n# downloading data sets\n!gdown --id 1lrT0Ub2QoJ3f3rHpRmFYRwuTUfh7bTUI # train set\n!gdown --id 1dYF26xgFpoUtnBDmhYWMcvyX_qEeAPDp # validation set\n!gdown --id 1YusblOsvP77d10Vw3TUUGcSbRwSVK3Ep # holdout set\n","metadata":{"id":"NXjU1MfGSt4c","outputId":"fcaf1b2a-544c-4fe1-b38a-01b40c501e68","execution":{"iopub.status.busy":"2022-07-26T08:14:07.380904Z","iopub.execute_input":"2022-07-26T08:14:07.381234Z","iopub.status.idle":"2022-07-26T08:14:32.923996Z","shell.execute_reply.started":"2022-07-26T08:14:07.381199Z","shell.execute_reply":"2022-07-26T08:14:32.922821Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Data loader with data augmentation\n\nclass H5FileDataset(Dataset):\n    # dataloader output: (pic_indices, color_channel, height, width)\n    def __init__(self, h5_filename, transform=None, target_transform=None):\n        self.h5_filename = h5_filename\n        self.img_h5_file = self._load_h5_file(self.h5_filename)\n        self.all_labels = self.img_h5_file['labels'][:]\n        self.transform = transform  # it just assign the \"state\" of transform, not apply it\n\n\n    def __len__(self):\n        return len(self.all_labels)\n\n\n    def __getitem__(self, idx):\n        img = self.img_h5_file['img_data'][idx]\n        label = self.img_h5_file['labels'][idx]\n\n        label = torch.as_tensor(label, dtype=torch.float64)\n\n        img = np.transpose(img, [2, 0, 1])\n        img = img.astype(np.double)\n        img = torch.as_tensor(img, dtype=torch.float64)\n        # img = img/255\n        if self.transform is not None:\n            img = self.transform(img)  # we're going to write specific methods for transform\n        return img, label\n\n\n    def _load_h5_file(self, h5_filename):\n        file = h5py.File(h5_filename, 'r')\n        img_data = file['pic_mat']\n        img_labels = file['labels']\n        return dict(file=file, img_data=img_data, labels=img_labels)\n","metadata":{"id":"rsgC7zmRleKV","execution":{"iopub.status.busy":"2022-07-26T08:14:47.902965Z","iopub.execute_input":"2022-07-26T08:14:47.903322Z","iopub.status.idle":"2022-07-26T08:14:47.914365Z","shell.execute_reply.started":"2022-07-26T08:14:47.903294Z","shell.execute_reply":"2022-07-26T08:14:47.912994Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# hyper-parameters\nuse_cuda = torch.cuda.is_available()\nbest_acc = 0  # best test accuracy\nstart_epoch = 0  # start from epoch 0 or last checkpoint epoch\nbatch_size = 32\nmax_epochs = 25  # Please change this to 200\n\nbase_learning_rate = 0.0005\n\ntorchvision_transforms = True  # True/False if you want use torchvision augmentations\n\ndef train(net, epoch, trainloader, use_cuda=True):\n  #print('\\nEpoch: %d' % epoch)\n  net.train()\n  train_loss = 0\n  correct = 0\n  total = 0\n  for batch_idx, (inputs, targets) in enumerate(trainloader):\n    targets = np.array(targets, dtype=int)\n    targets = torch.as_tensor(targets, dtype=torch.long)\n    if use_cuda:\n      inputs, targets = inputs.cuda(), targets.cuda()\n\n    optimizer.zero_grad()\n    inputs, targets = Variable(inputs), Variable(targets)\n    outputs = net(inputs)\n    loss = criterion(outputs, targets)\n    loss.backward()\n\n    optimizer.step()\n\n    train_loss += loss.item()\n    _, predicted = torch.max(outputs.data, 1)\n    total += targets.size(0)\n    correct += predicted.eq(targets.data).cpu().sum()\n\n      \n  return (train_loss/batch_idx, 100.*correct/total)\n\n\ndef test(net, epoch, testloader, outModelName, use_cuda=True):\n  global best_acc\n  net.eval()\n  test_loss, correct, total = 0, 0, 0\n  with torch.no_grad():\n    for batch_idx, (inputs, targets) in enumerate(testloader):\n      targets = np.array(targets, dtype=int)\n      targets = torch.as_tensor(targets, dtype=torch.long)\n      if use_cuda:\n        inputs, targets = inputs.cuda(), targets.cuda()\n\n      outputs = net(inputs)\n      loss = criterion(outputs, targets)\n\n      test_loss += loss.item()\n      _, predicted = torch.max(outputs.data, 1)\n      total += targets.size(0)\n      correct += predicted.eq(targets.data).cpu().sum()\n\n      if batch_idx % 200 == 0:\n        print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n            % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n    # Save checkpoint.\n    acc = 100.*correct/total\n    if acc > best_acc:\n        best_acc = acc\n        checkpoint(net, acc, epoch, outModelName)\n    return (test_loss/batch_idx, 100.*correct/total)\n\n# checkpoint & adjust_learning_rate\ndef checkpoint(model, acc, epoch, outModelName):\n  # Save checkpoint.\n  print('Saving..')\n  state = {\n      'state_dict': model.state_dict(),\n      'acc': acc,\n      'epoch': epoch,\n      'rng_state': torch.get_rng_state()\n  }\n  if not os.path.isdir('checkpoint'):\n      os.mkdir('checkpoint')\n  torch.save(state, f'./checkpoint/{outModelName}.t7')\n\ndef adjust_learning_rate(optimizer, epoch):\n  \"\"\"decrease the learning rate at 100 and 150 epoch\"\"\"\n  lr = base_learning_rate\n  if epoch <= 9 and lr > 0.1:\n    # warm-up training for large minibatch\n    lr = 0.1 + (base_learning_rate - 0.1) * epoch / 10.\n  if epoch >= 100:\n    lr /= 10\n  if epoch >= 150:\n    lr /= 10\n  for param_group in optimizer.param_groups:\n    param_group['lr'] = lr\n","metadata":{"id":"ZFpQN-iaodIU","execution":{"iopub.status.busy":"2022-07-26T08:53:07.445794Z","iopub.execute_input":"2022-07-26T08:53:07.446150Z","iopub.status.idle":"2022-07-26T08:53:07.467178Z","shell.execute_reply.started":"2022-07-26T08:53:07.446120Z","shell.execute_reply":"2022-07-26T08:53:07.466143Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"composed = transforms.Compose([transforms.RandomCrop([500, 500]), transforms.Resize([160, 160])])\n\n\ntrainset = H5FileDataset('kdef_train_dataset.h5', transform=composed)\ntestset = H5FileDataset('kdef_val_dataset.h5', transform=composed)\nholdoutset = H5FileDataset('kdef_test_dataset.h5', transform=composed)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\nholdoutloader = torch.utils.data.DataLoader(holdoutset, batch_size=batch_size, shuffle=True)\n","metadata":{"id":"gGyEBgdFc5OY","execution":{"iopub.status.busy":"2022-07-26T08:16:26.011394Z","iopub.execute_input":"2022-07-26T08:16:26.012397Z","iopub.status.idle":"2022-07-26T08:16:26.023860Z","shell.execute_reply.started":"2022-07-26T08:16:26.012349Z","shell.execute_reply":"2022-07-26T08:16:26.022821Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# load FaceNet\n# net = InceptionResnetV1(pretrained='vggface2', classify=True)\n# net # shows structure of network","metadata":{"id":"tzwmqwImlzkH","execution":{"iopub.status.busy":"2022-07-25T08:19:12.646222Z","iopub.execute_input":"2022-07-25T08:19:12.646867Z","iopub.status.idle":"2022-07-25T08:19:12.653859Z","shell.execute_reply.started":"2022-07-25T08:19:12.646832Z","shell.execute_reply":"2022-07-25T08:19:12.652841Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# in_ftrs = net.logits.in_features # number of input features of the last layer of the classifier\n# print(\"+ Previous Nr of Outputs: \" + str(net.logits.out_features)) # number of output features of the last layer of the classifier == num_classes\n# net.logits = nn.Linear(in_ftrs, 2, )\n# print(\"++++++ New Nr of Outputs: \" + str(net.logits.out_features))\n\n# in_ftrs = net.last_linear.in_features # number of input features of the last layer of the classifier\n# print(\"+ Previous Nr of Outputs: \" + str(net.last_linear.out_features)) # number of output features of the last layer of the classifier == num_classes\n# net.last_linear = nn.Linear(in_ftrs, 2, )\n# print(\"++++++ New Nr of Outputs: \" + str(net.last_linear.out_features))","metadata":{"id":"s5-VVeEdzXbs","outputId":"f554156f-c22d-4913-add1-a2229b6d0f92","execution":{"iopub.status.busy":"2022-07-25T08:19:12.657746Z","iopub.execute_input":"2022-07-25T08:19:12.658280Z","iopub.status.idle":"2022-07-25T08:19:12.663478Z","shell.execute_reply.started":"2022-07-25T08:19:12.658256Z","shell.execute_reply":"2022-07-25T08:19:12.662198Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"net = InceptionResnetV1(pretrained='vggface2', num_classes = 2, classify=True)\n\nfor param in net.parameters():\n  param.requires_grad = False\n\nin_ftrs = net.logits.in_features # number of input features of the last layer of the classifier\nnet.logits = nn.Linear(in_ftrs, 2)\n\n\nnet = net.double()\nnet = net.cuda()\nresult_folder = './results/'\nif not os.path.exists(result_folder):\n    os.makedirs(result_folder)\n\nlogname = result_folder + net.__class__.__name__ + '_vgg_face2_to_kdef' + '__lr_' + str(base_learning_rate) + '__bs_' + str(batch_size) + '_full-retrain.csv'\n\n# Optimizer and criterion\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=base_learning_rate, momentum=0.9, weight_decay=5e-4)\n\noutModelName = 'pretrain'\nif not os.path.exists(logname):\n    with open(logname, 'w') as logfile:\n        logwriter = csv.writer(logfile, delimiter=',')\n        logwriter.writerow(['epoch', 'train loss', 'train acc', 'test loss', 'test acc'])\n\nfor epoch in range(start_epoch, max_epochs):\n    adjust_learning_rate(optimizer, epoch)\n    #print('finished adjust lr')\n    train_loss, train_acc = train(net, epoch, trainloader, use_cuda=use_cuda)\n    #print('finished train epoch ', epoch)\n    test_loss, test_acc = test(net, epoch, testloader, outModelName, use_cuda=use_cuda)\n    #print('finished test epoch ', epoch)\n    with open(logname, 'a') as logfile:\n        logwriter = csv.writer(logfile, delimiter=',')\n        logwriter.writerow([epoch, train_loss, train_acc.item(), test_loss, test_acc.item()])\n    print(f'Epoch: {epoch} | train acc: {train_acc} | test acc: {test_acc}')","metadata":{"id":"7BYCTgZpBoOF","outputId":"f5d62b7b-5161-4e8c-e9e0-16d9d2fd2733","execution":{"iopub.status.busy":"2022-07-26T08:53:26.592022Z","iopub.execute_input":"2022-07-26T08:53:26.592572Z","iopub.status.idle":"2022-07-26T08:55:34.587877Z","shell.execute_reply.started":"2022-07-26T08:53:26.592538Z","shell.execute_reply":"2022-07-26T08:55:34.586051Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# title plot results\n#results = pd.read_csv(f'./results/InceptionResnetV1_{outModelName}.csv', sep =',')\nresults = pd.read_csv(logname, sep =',')\nresults.head()\n\ntrain_accuracy = results['train acc'].values\ntest_accuracy = results['test acc'].values\n","metadata":{"id":"B1jgyVjH_cSx","execution":{"iopub.status.busy":"2022-07-26T08:58:53.967889Z","iopub.execute_input":"2022-07-26T08:58:53.968294Z","iopub.status.idle":"2022-07-26T08:58:53.978443Z","shell.execute_reply.started":"2022-07-26T08:58:53.968267Z","shell.execute_reply":"2022-07-26T08:58:53.977302Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"figureName = logname[:-4] + \".png\"\n\nplt.plot(results['epoch'].values, train_accuracy, label='train')\nplt.plot(results['epoch'].values, test_accuracy, label='test')\nplt.xlabel('Number of epochs')\nplt.ylabel('Accuracy')\nplt.title(f'Train/Test Accuracy curve for {max_epochs} epochs')\nplt.savefig(figureName)\nplt.legend()\nplt.show()","metadata":{"id":"4jxsjtnZDaE6","outputId":"f2a55f14-582c-4510-b6b6-840e13495712","execution":{"iopub.status.busy":"2022-07-26T08:58:57.905714Z","iopub.execute_input":"2022-07-26T08:58:57.906064Z","iopub.status.idle":"2022-07-26T08:58:58.134458Z","shell.execute_reply.started":"2022-07-26T08:58:57.906035Z","shell.execute_reply":"2022-07-26T08:58:58.133541Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def holdout_check(net, holdoutloader, use_cuda=True):\n  global best_acc\n  net.eval()\n  test_loss, correct, total = 0, 0, 0\n  with torch.no_grad():\n    for batch_idx, (inputs, targets) in enumerate(testloader):\n      targets = np.array(targets, dtype=int)\n      targets = torch.as_tensor(targets, dtype=torch.long)\n      if use_cuda:\n        inputs, targets = inputs.cuda(), targets.cuda()\n\n      outputs = net(inputs)\n      loss = criterion(outputs, targets)\n\n      test_loss += loss.item()\n      _, predicted = torch.max(outputs.data, 1)\n      total += targets.size(0)\n      correct += predicted.eq(targets.data).cpu().sum()\n    \n  print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n        % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n  \n  return (test_loss/batch_idx, 100.*correct/total)","metadata":{"id":"NeGJp7S8EY9R","outputId":"c0c33227-9b0a-4e0e-964e-e45f3caa9cc1","execution":{"iopub.status.busy":"2022-07-26T08:36:56.051396Z","iopub.execute_input":"2022-07-26T08:36:56.051877Z","iopub.status.idle":"2022-07-26T08:36:56.061177Z","shell.execute_reply.started":"2022-07-26T08:36:56.051840Z","shell.execute_reply":"2022-07-26T08:36:56.060219Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"holdout_check(net, holdoutloader, use_cuda=use_cuda)","metadata":{"id":"M3ZvjxjqJYSo","execution":{"iopub.status.busy":"2022-07-26T08:59:52.687837Z","iopub.execute_input":"2022-07-26T08:59:52.688196Z","iopub.status.idle":"2022-07-26T08:59:54.309764Z","shell.execute_reply.started":"2022-07-26T08:59:52.688167Z","shell.execute_reply":"2022-07-26T08:59:54.308779Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"./checkpoint/pretrain.t7\"> Download File </a>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}